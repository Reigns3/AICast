{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54941f4b-32de-43b2-8987-61212b6ec4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 458MB\n",
      "Dimensions:    (time: 35064, latitude: 11, longitude: 33)\n",
      "Coordinates:\n",
      "    number     int64 8B ...\n",
      "  * time       (time) datetime64[ns] 281kB 2021-01-01 ... 2024-12-31T23:00:00\n",
      "    surface    float64 8B ...\n",
      "  * latitude   (latitude) float64 88B 49.0 48.75 48.5 48.25 ... 47.0 46.75 46.5\n",
      "  * longitude  (longitude) float64 264B 9.5 9.75 10.0 10.25 ... 17.0 17.25 17.5\n",
      "Data variables:\n",
      "    d2m        (time, latitude, longitude) float32 51MB ...\n",
      "    t2m        (time, latitude, longitude) float32 51MB ...\n",
      "    sp         (time, latitude, longitude) float32 51MB ...\n",
      "    u10        (time, latitude, longitude) float32 51MB ...\n",
      "    v10        (time, latitude, longitude) float32 51MB ...\n",
      "    tcc        (time, latitude, longitude) float32 51MB ...\n",
      "    cp         (time, latitude, longitude) float32 51MB ...\n",
      "    lsp        (time, latitude, longitude) float32 51MB ...\n",
      "    tp         (time, latitude, longitude) float32 51MB ...\n",
      "Attributes:\n",
      "    GRIB_edition:            1\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-03-31T01:13 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "# Load dataset\n",
    "file_path = \"Dataset/v4_final_dataset/final_weather_data.nc\"\n",
    "dataset = xr.open_dataset(file_path)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd6b28a-b4b5-4cf6-a013-775779bc9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (35064, 11, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "variables = ['d2m', 't2m', 'sp', 'u10', 'v10', 'tcc', 'cp', 'lsp', 'tp']\n",
    "data_array = np.stack([dataset[var].values for var in variables], axis=-1)\n",
    "# Get dimensions\n",
    "n_samples, n_lat, n_lon = data_array.shape[0], dataset.latitude.size, dataset.longitude.size\n",
    "n_vars = len(variables)\n",
    "print(f\"Raw data shape: {data_array.shape}\")  #(35064, 11, 33, 9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8028bea2-96b1-4227-bc46-abce837079d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned t2m min/max: 242.87K / 310.65K\n",
      "Adjusted sp min/max: 727.33 hPa / 1030.46 hPa\n",
      "Cleaned tp min/max: 0.00 mm / 20.46 mm\n",
      "Cleaned tcc min/max: 0.00 / 1.00\n"
     ]
    }
   ],
   "source": [
    "# Clean all variables\n",
    "temp_idx = variables.index('t2m')\n",
    "sp_idx = variables.index('sp')\n",
    "precip_indices = [variables.index(var) for var in ['tp', 'cp', 'lsp']]\n",
    "cloud_idx = variables.index('tcc')\n",
    "\n",
    "# Clean temperature\n",
    "data_array[..., temp_idx] = np.where(data_array[..., temp_idx] <= 0, 250.0, data_array[..., temp_idx])\n",
    "print(f\"Cleaned t2m min/max: {np.min(data_array[..., temp_idx]):.2f}K / {np.max(data_array[..., temp_idx]):.2f}K\")\n",
    "\n",
    "# Clean surface pressure (convert Pa to hPa)\n",
    "data_array[..., sp_idx] = data_array[..., sp_idx] / 100\n",
    "print(f\"Adjusted sp min/max: {np.min(data_array[..., sp_idx]):.2f} hPa / {np.max(data_array[..., sp_idx]):.2f} hPa\")\n",
    "\n",
    "# Clean precipitation (convert m to mm and set negative to 0)\n",
    "for idx in precip_indices:\n",
    "    data_array[..., idx] = np.where(data_array[..., idx] < 0, 0.0, data_array[..., idx] * 1000)  # Convert m to mm\n",
    "print(f\"Cleaned tp min/max: {np.min(data_array[..., precip_indices[0]]):.2f} mm / {np.max(data_array[..., precip_indices[0]]):.2f} mm\")\n",
    "\n",
    "# Clean cloud cover (clip to [0, 1])\n",
    "data_array[..., cloud_idx] = np.clip(data_array[..., cloud_idx], 0.0, 1.0)\n",
    "print(f\"Cleaned tcc min/max: {np.min(data_array[..., cloud_idx]):.2f} / {np.max(data_array[..., cloud_idx]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c796555f-8553-4faa-9137-4b538cb5dd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened data shape: (12728232, 9)\n",
      "Scaler n_features_in_: 9\n",
      "Scaler data_min_: [230.5844   242.86975  727.32623   -8.344498 -13.136108   0.\n",
      "   0.         0.         0.      ]\n",
      "Scaler data_max_: [2.9779272e+02 3.1065259e+02 1.0304587e+03 1.2643265e+01 1.1074310e+01\n",
      " 1.0000000e+00 1.3957739e+01 1.6366959e+01 2.0457029e+01]\n",
      "Normalized data shape: (35064, 11, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "# Reshape for single scaler: (n_samples * n_lat * n_lon, n_vars)\n",
    "data_flat = data_array.reshape(-1, n_vars)  # Shape: (n_samples * 11 * 33, 9)\n",
    "print(f\"Flattened data shape: {data_flat.shape}\")  # Should be (35064 * 11 * 33, 9)\n",
    "\n",
    "# Fit and transform with a single scaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data_flat = scaler.fit_transform(data_flat)\n",
    "print(f\"Scaler n_features_in_: {scaler.n_features_in_}\")  # Should be 9\n",
    "print(f\"Scaler data_min_: {scaler.data_min_}\")  # Expected: min per variable\n",
    "print(f\"Scaler data_max_: {scaler.data_max_}\")  # Expected: max per variable\n",
    "\n",
    "# Reshape back to original\n",
    "normalized_data = normalized_data_flat.reshape(n_samples, n_lat, n_lon, n_vars)\n",
    "print(f\"Normalized data shape: {normalized_data.shape}\")  # Should be (35064, 11, 33, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ee319d-f546-4e28-9e49-17aa7e677887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler and data\n",
    "joblib.dump(scaler, 'data/scaler.pkl')\n",
    "np.save('data/normalized_data_single_.npy', normalized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00bc3bf-9ff6-4d43-9ba1-695ea7659e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (24544, 11, 33, 9)\n",
      "Validation data shape: (5260, 11, 33, 9)\n",
      "Test data shape: (5260, 11, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "# Split and save test data (adjust ratios as needed)\n",
    "train_split = int(0.7 * n_samples)\n",
    "val_split = int(0.85 * n_samples)\n",
    "train_data = normalized_data[:train_split]\n",
    "val_data = normalized_data[train_split:val_split]\n",
    "test_data = normalized_data[val_split:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")  # ~24,545 hours\n",
    "print(f\"Validation data shape: {val_data.shape}\")  # ~5,260 hours\n",
    "print(f\"Test data shape: {test_data.shape}\")  # ~5,260 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd359f6c-97cf-42ca-b213-d97f0c43281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X, y pairs for model training (using 24-hour sequences)\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])  # Input sequence\n",
    "        y.append(data[i+seq_length])    # Target value (next timestep)\n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "X_train, y_train = create_sequences(train_data, 24)\n",
    "X_val, y_val = create_sequences(val_data, 24)\n",
    "X_test, y_test = create_sequences(test_data, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e00963b-edc4-4ff0-b087-085c0b43c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Saved datasets with shapes:\n",
      "X_train: (24520, 24, 11, 33, 9) y_train: (24520, 11, 33, 9)\n",
      "X_val: (5236, 24, 11, 33, 9) y_val: (5236, 11, 33, 9)\n",
      "X_test: (5236, 24, 11, 33, 9) y_test: (5236, 11, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "# After creating and splitting and transposing sequences\n",
    "\n",
    "# Step 9: Save Data\n",
    "np.save('data/X_train.npy', X_train)\n",
    "np.save('data/y_train.npy', y_train)\n",
    "np.save('data/X_val.npy', X_val)\n",
    "np.save('data/y_val.npy', y_val)\n",
    "np.save('data/X_test.npy', X_test)\n",
    "np.save('data/y_test.npy', y_test)\n",
    "\n",
    "print(\"Preprocessing complete. Saved datasets with shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ff4a72-5dc2-4f82-9ca0-4fc048de2f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_small shape: (318, 24, 11, 33, 9)\n"
     ]
    }
   ],
   "source": [
    "# Smaller Testset for deployment\n",
    "\n",
    "X_test = np.load('data/X_test.npy')  # (5236, 24, 11, 33, 9)\n",
    "X_test_small = X_test[-318:]  # Last 318 sequences (~99.8 MB)\n",
    "np.save('data/X_test_small.npy', X_test_small)\n",
    "print(f\"X_test_small shape: {X_test_small.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
